from kokoro import KPipeline
import sounddevice as sd
import numpy as np
import torch
# ðŸ‡ºðŸ‡¸ 'a' => American English, ðŸ‡¬ðŸ‡§ 'b' => British English
# ðŸ‡ªðŸ‡¸ 'e' => Spanish es
# ðŸ‡«ðŸ‡· 'f' => French fr-fr
# ðŸ‡®ðŸ‡³ 'h' => Hindi hi
# ðŸ‡®ðŸ‡¹ 'i' => Italian it
# ðŸ‡¯ðŸ‡µ 'j' => Japanese: pip install misaki[ja]
# ðŸ‡§ðŸ‡· 'p' => Brazilian Portuguese pt-br
# ðŸ‡¨ðŸ‡³ 'z' => Mandarin Chinese: pip install misaki[zh]
pipeline = KPipeline(lang_code='z', repo_id='hexgrad/Kokoro-82M-v1.1-zh') # <= make sure lang_code matches voice, reference above.
# voice_tensor = torch.load('.\Speak\model\yln\mygf-yln.pth', weights_only=True, map_location=torch.device('cpu')) # Training!
# kokoro haven't push the train codeâ€¦â€¦


def speak(text, speed=1):
    generator = pipeline(
        text, voice='af_maple',
        speed=speed, split_pattern=r'\n+'
    )
    
    audio_chunks = []
    for i, (gs, ps, audio) in enumerate(generator):
        print(f"ç”Ÿæˆè¯­éŸ³æ®µ {i}: {gs}")
        audio_chunks.append(audio)
    
    full_audio = np.concatenate(audio_chunks)

    sd.play(full_audio, samplerate=24000)
    sd.wait()